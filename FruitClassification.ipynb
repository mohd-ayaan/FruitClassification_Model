{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is able to recognize fresh and rotten fruit. I achieved the model to a validation accuracy of `92%`, I have used some combination of transfer learning, data augmentation, and fine tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.io as tv_io\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The dataset comes from [Kaggle](https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification). The dataset structure is in the `data/fruits` folder. There are 6 categories of fruits: fresh apples, fresh oranges, fresh bananas, rotten apples, rotten oranges, and rotten bananas. This will mean that your model will require an output layer of 6 neurons to do the categorization successfully. we'll also need to compile the model with `categorical_crossentropy`, as we have more than two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fruits.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Load ImageNet Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "started with a model pretrained on ImageNet. Loaded the model with the correct weights. Because these pictures are in color, there will be three channels for red, green, and blue. I've filled in the input shape. A reference for setting up the pretrained model, please take a look at [notebook 05b](05b_presidential_doggy_door.ipynb) where transfer learning is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\amohd/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528M/528M [00:18<00:00, 29.3MB/s] \n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "weights = VGG16_Weights.IMAGENET1K_V1\n",
    "vgg_model = vgg16(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Freeze Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Freezing the base model, as done in [notebook 05b](05b_presidential_doggy_door.ipynb). This is done so that all the learning from the ImageNet dataset does not get destroyed in the initial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze base model\n",
    "vgg_model.requires_grad_(False)\n",
    "next(iter(vgg_model.parameters())).requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Add Layers to Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to add layers to the pretrained model. [Notebook 05b](05b_presidential_doggy_door.ipynb) is used as a guide. Payed close attention to the last dense layer and make sure it has the correct number of neurons to classify the different types of fruit.\n",
    "\n",
    "The later layers of a model become more specific to the data the model trained on. Since we want the more general learnings from VGG, we can select parts of it, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.classifier[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've taken what we've wanted from VGG16, then we can add our own modifications. No matter what additional modules we add, we still need to end with one value for each output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (2): Flatten(start_dim=1, end_dim=-1)\n",
       "  (3): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (4): Linear(in_features=4096, out_features=500, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_CLASSES = 6\n",
    "\n",
    "my_model = nn.Sequential(\n",
    "    vgg_model.features,\n",
    "    vgg_model.avgpool,\n",
    "    nn.Flatten(),\n",
    "    vgg_model.classifier[0:3],\n",
    "    nn.Linear(4096, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, N_CLASSES)\n",
    ")\n",
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to compile the model with loss and metrics options. We have 6 classes, so CrossEntropyLoss() loss function is used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the entire base model for fine-tuning\n",
    "vgg_model.requires_grad_(True)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer with a very low learning rate for fine-tuning\n",
    "optimizer = Adam(my_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Optional: learning rate scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Data Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess input images, we use the transforms included with the VGG16 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trans = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly augmented the data to improve the dataset. look at [notebook 04a](04a_asl_augmentation.ipynb) and [notebook 05b](05b_presidential_doggy_door.ipynb) for augmentation examples. There is also documentation for the [TorchVision Transforms class](https://pytorch.org/vision/stable/transforms.html).\n",
    "\n",
    "**Hint**: Remember not to make the data augmentation too extreme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH, IMG_HEIGHT = (224, 224)\n",
    "\n",
    "random_trans = transforms.Compose([\n",
    "    transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToDtype(torch.float32, scale=True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to load the train and validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LABELS = [\"freshapples\", \"freshbanana\", \"freshoranges\", \"rottenapples\", \"rottenbanana\", \"rottenoranges\"] \n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for l_idx, label in enumerate(DATA_LABELS):\n",
    "            data_paths = glob.glob(data_dir + label + '/*.png', recursive=True)\n",
    "            for path in data_paths:\n",
    "                img = tv_io.read_image(path, tv_io.ImageReadMode.RGB)\n",
    "                self.imgs.append(pre_trans(img).to(device))\n",
    "                self.labels.append(torch.tensor(l_idx).to(device))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the batch size `n` and set `shuffle` either to `True` or `False` depending on if we are `train`ing or `valid`ating. For a reference, check out [notebook 05b](05b_presidential_doggy_door.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 32\n",
    "\n",
    "train_path = \"data/fruits/train/\"\n",
    "train_data = MyDataset(train_path)\n",
    "train_loader = DataLoader(train_data, batch_size=n, shuffle=True)\n",
    "train_N = len(train_loader.dataset)\n",
    "\n",
    "valid_path = \"data/fruits/valid/\"\n",
    "valid_data = MyDataset(valid_path)\n",
    "valid_loader = DataLoader(valid_data, batch_size=n, shuffle=False)\n",
    "valid_N = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model! I've moved the `train` and `validate` functions to our [utils.py](./utils.py) file. Before running the below, all the variables are correctly defined.\n",
    "It may help to rerun this cell or change the number of `epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 70.9824 Accuracy: 0.6946\n",
      "Valid - Loss: 16.3028 Accuracy: 0.7781\n",
      "Epoch: 1\n",
      "Train - Loss: 21.5779 Accuracy: 0.8130\n",
      "Valid - Loss: 11.0210 Accuracy: 0.7690\n",
      "Epoch: 2\n",
      "Train - Loss: 16.2375 Accuracy: 0.8604\n",
      "Valid - Loss: 5.5058 Accuracy: 0.8693\n",
      "Epoch: 3\n",
      "Train - Loss: 13.0727 Accuracy: 0.8900\n",
      "Valid - Loss: 9.1529 Accuracy: 0.8359\n",
      "Epoch: 4\n",
      "Train - Loss: 9.0874 Accuracy: 0.9188\n",
      "Valid - Loss: 8.7047 Accuracy: 0.8663\n",
      "Epoch: 5\n",
      "Train - Loss: 7.9512 Accuracy: 0.9255\n",
      "Valid - Loss: 4.6246 Accuracy: 0.8936\n",
      "Epoch: 6\n",
      "Train - Loss: 9.7744 Accuracy: 0.9103\n",
      "Valid - Loss: 3.6779 Accuracy: 0.9027\n",
      "Epoch: 7\n",
      "Train - Loss: 7.8933 Accuracy: 0.9239\n",
      "Valid - Loss: 3.9176 Accuracy: 0.8997\n",
      "Epoch: 8\n",
      "Train - Loss: 6.1387 Accuracy: 0.9467\n",
      "Valid - Loss: 3.8745 Accuracy: 0.9210\n",
      "Epoch: 9\n",
      "Train - Loss: 5.5798 Accuracy: 0.9492\n",
      "Valid - Loss: 4.8223 Accuracy: 0.9301\n",
      "Epoch: 10\n",
      "Train - Loss: 5.9077 Accuracy: 0.9501\n",
      "Valid - Loss: 3.2444 Accuracy: 0.9331\n",
      "Epoch: 11\n",
      "Train - Loss: 8.7027 Accuracy: 0.9349\n",
      "Valid - Loss: 5.7781 Accuracy: 0.8906\n",
      "Epoch: 12\n",
      "Train - Loss: 8.0728 Accuracy: 0.9272\n",
      "Valid - Loss: 4.1092 Accuracy: 0.9027\n",
      "Epoch: 13\n",
      "Train - Loss: 8.6664 Accuracy: 0.9255\n",
      "Valid - Loss: 8.9959 Accuracy: 0.8541\n",
      "Epoch: 14\n",
      "Train - Loss: 6.9000 Accuracy: 0.9492\n",
      "Valid - Loss: 4.0729 Accuracy: 0.9210\n",
      "Epoch: 15\n",
      "Train - Loss: 10.5199 Accuracy: 0.9027\n",
      "Valid - Loss: 16.2552 Accuracy: 0.7416\n",
      "Epoch: 16\n",
      "Train - Loss: 9.1875 Accuracy: 0.9247\n",
      "Valid - Loss: 3.1547 Accuracy: 0.9240\n",
      "Epoch: 17\n",
      "Train - Loss: 4.7276 Accuracy: 0.9611\n",
      "Valid - Loss: 4.5953 Accuracy: 0.8875\n",
      "Epoch: 18\n",
      "Train - Loss: 4.7066 Accuracy: 0.9569\n",
      "Valid - Loss: 13.0732 Accuracy: 0.8359\n",
      "Epoch: 19\n",
      "Train - Loss: 5.2716 Accuracy: 0.9484\n",
      "Valid - Loss: 10.5635 Accuracy: 0.8663\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9 Unfreeze Model for Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If reached 92% validation accuracy already, next step is optional. If not, fine tuning the model with a very low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "vgg_model.requires_grad_(True)\n",
    "optimizer = Adam(my_model.parameters(), lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 2.4129 Accuracy: 0.9772\n",
      "Valid - Loss: 6.2580 Accuracy: 0.9088\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.10 Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we've a model that has a validation accuracy of 92% or higher. If not, you may want to go back and either run more epochs of training, or adjust your data augmentation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid - Loss: 6.2580 Accuracy: 0.9088\n"
     ]
    }
   ],
   "source": [
    "utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
